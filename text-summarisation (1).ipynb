{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project: Text Summarisation\n## Installing the required libraries \nFirst we will install all the required libraries that we are going to use in building our __Neural Network__.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -U transformers\n!pip install -U accelerate \n!pip install -U datasets\n!pip install -U bertviz\n!pip install -U umap-learn\n!pip install -U sentencepiece\n!pip install -U urllib3\n!pip install py7zr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the dataset\nHere we will now load the Article data set into our code to be done summarisation on. The data set used here is the __CNN daily mail__ data set in which we have 312k rows of articles published on the daily basis.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['train'][1]['article'][:350]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['train'][1]['highlights']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Selecting Transformer\nHere at this step, we will see which of the four;\n1. gpt2-medium\n2. t5-base\n3. facebook/bart-large-cnn\n4. google/pegasus-cnn_dailymail\n\nWorks best in generating summaries in order to be transformed into a model for Text Summarisation.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\npipe = pipeline(\"text-generation\", model = \"gpt2-medium\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['train'][1]['article'][:2000]\ninput_text = dataset['train'][1]['article'][:2000]\nquery = input_text + \"\\nTL; DR:\\n\"\npipe_out = pipe(query, max_length = 512, clean_up_tokenization_spaces = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe_out[0]['generated_text'][len(query):]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summaries = {}\nsummaries['gpt2-medium-380M'] = pipe_out[0]['generated_text'][len(query):]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Try out T5 transformers\npipe = pipeline('summarization', model = 't5-base')\npipe_out = pipe(input_text)\nsummaries['t5-base-223M'] = pipe_out[0]['summary_text']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline('summarization', model = 'facebook/bart-large-cnn')\npipe_out = pipe(input_text)\nsummaries['bart-large-cnn-400M'] = pipe_out[0]['summary_text']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline('summarization', model = 'google/pegasus-cnn_dailymail')\npipe_out = pipe(input_text)\nsummaries['pegasus-cnn-568M'] = pipe_out[0]['summary_text']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for model in summaries:\n    print(model.upper())\n    print(summaries[model])\n    print(\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the BART dataset is giving much more cleare and accurate results so we will move ahead with this model for our text summarization problem for a conversational data set.","metadata":{}},{"cell_type":"markdown","source":"## Importing basic libaries \nImporting the basic libraries for building the Text Summarisation model.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import pipeline\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting the device\nHere we will set the device configuration for data processing, such as base model to be used, tokenizing the data, gpu setting for batch processing and faster rendering of the code.","metadata":{}},{"cell_type":"code","source":"device = 'gpu'\nmodel_ckpt = 'facebook/bart-large-cnn'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the data\nHere we will use the __Samsum__ data set from __Hugging Face__ which consists of __Dialogues__ and __Summary__ of the respective dialogues held in a conversation over the text chat.","metadata":{}},{"cell_type":"code","source":"samsun = load_dataset('samsum')\nsamsun","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"samsun['train'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visual Representation\nPlotting the histogram to check the maximum length of the __Dialogues__ and the __Summaries__","metadata":{}},{"cell_type":"code","source":"dialogue_len  = [len(x['dialogue'].split()) for x in samsun['train']]\nsummary_len =  [len(x['summary'].split()) for x in samsun['train']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndata = pd.DataFrame([dialogue_len, summary_len]).T\ndata.columns = ['Dialogue Length', 'Summary Length']\n\ndata\n\ndata.hist(figsize = (15,5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we see that the maximum length of a __Dialogue__ is not more than __500__ and the maximum length of the generated summary is also less than __70__ words.","metadata":{}},{"cell_type":"code","source":"# lets build the DATA COLLATOR\ndef get_feature(batch):\n    encodings = tokenizer(batch['dialogue'], text_target = batch['summary'], max_length = 1024, truncation = True)\n    encodings = {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask'], 'labels': encodings['labels']}\n    return encodings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"samsun_pt = samsun.map(get_feature, batched = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"samsun_pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = ['input_ids', 'labels', 'attention_mask']\nsamsun_pt.set_format(type = 'torch', columns = columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model = model)\ntraining_args = TrainingArguments(\n    output_dir = 'bart_samsum',\n    num_train_epochs = 1,\n    per_device_train_batch_size = 4,\n    per_device_eval_batch_size = 4,\n    weight_decay = 0.01,\n    logging_steps = 10,\n    eval_strategy = 'steps',\n    eval_steps = 500,\n    save_steps = 1e6,\n    gradient_accumulation_steps = 16\n)\ntrainer = Trainer(model = model, args = training_args, processing_class= = tokenizer, data_collator = data_collator, train_dataset = samsun_pt['train'], eval_dataset = samsun_pt['validation'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the model\nNow since we trained the model on the provided data, so in order to use the same model to perform the same task of Text Summarisation we need not to run the whole code again, we just have to save the model. Giving a custom name to the saved model.","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"Text_summarization_2ndProject\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing the model on the new data\nNow since we have made a machine learning model that summarizes text, so in order to see its flexibility we will run it on the new data and test its validity that if it is working with the same precision as for the previous data or not.","metadata":{}},{"cell_type":"code","source":"#custom Dialogue Prediction\n\npipe = pipeline('summarization', model = 'Text_summarization_2ndProject')\ngen_kwargs = {'length_penalty': 0.8, 'num_beams': 8, 'max_length': 128}\n\ncustom_dialogue = \"\"\"“Look what Eddie gave me,” said Cindy, all friendly. She pulled a pink teddy bear out of her purse and squeezed its belly. It sang “You Are My Sunshine” in a vibrating robot voice. “That’s nice,” said Jasmine, her voice so high that she sounded almost like the teddy bear. Cindy smiled and walked off with Eddie, swinging her hips back and forth.\n\"\"\"\n\nprint(pipe(custom_dialogue, **gen_kwargs))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, we can see the generated summary of the custom dialogue created, hence we can conclude that our Text Summarization model is working fine with new data as well.","metadata":{}}]}